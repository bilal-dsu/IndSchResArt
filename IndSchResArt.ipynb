{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Scholarly Research Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline for creating and analysing scientific networks. The first phase is downloading the publicly available data. The second phase is to extract the data of the provided date range. The third phase is to filter the cited and citing nodes of DOIs. The fourth phase is to load the data as scientific networks. Finally, the last phase is to index the articles.\n",
    "Publicly available dumps of both Crossref and COCI are free for use. Other than downloading the data, it is also needed to have seed DOIs. The steps are not dependent on each other but subsequent phases depend on data being placed in correct directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Profiles\\Users\\bilal.hayat\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase concerns extracting the metadata for an input date range. Only the past decade is considered to give equal chance to early career researchers. It is also needed to curtail processing all the data. The date range should be selected based on the available processing and RAM. To process large files, DASK is used for out-of-core processing. It can be scaled to multi-node clusters. The steps are not dependent on each other but subsequent phases depend on relevant data being extracted that can fit in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data directory to use\n",
    "DataDir = \"../DATA/\"\n",
    "\n",
    "# Input DASK configurations\n",
    "physicalCores = 8\n",
    "virtualPerCore = 2\n",
    "\n",
    "#Input year range\n",
    "start_year = 2010\n",
    "end_year = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory sub folders path\n",
    "path_to_crossref = DataDir+\"Crossref/crossref_public_data_file_2021_01/*.json.gz\"\n",
    "path_to_crossref_parquet = DataDir+\"Crossref/\"+str(start_year)+\"parquet\"+str(end_year)+\"/\"\n",
    "metadataFull = DataDir+\"Crossref/\"+str(start_year)+\"metadata\"+str(end_year)+\".pkl\"\n",
    "metadataDesc = DataDir+\"Crossref/\"+str(start_year)+\"desc\"+str(end_year)+\".pkl\"\n",
    "citNetFull = DataDir+\"COCI/\"+str(start_year)+\"COCI\"+str(end_year)+\".pkl\"\n",
    "\n",
    "CrossrefColsDF = ['DOI','Venue','Authors','Year']\n",
    "descColsDF = ['DOI','Title','Type','Subject']\n",
    "\n",
    "path_to_COCI = DataDir+\"COCI/csv/*.csv\"\n",
    "path_to_COCI_parquet = DataDir+\"COCI/\"+str(start_year)+\"parquet\"+str(end_year)+\"/\"\n",
    "COCIColsDDF=['citing','cited','creation', 'oci']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract metadata for the provided date range in parquet format\n",
    "This step relates to creating a parquet binary file format for Crossref JSON. It is needed for the fast processing of subsequent steps. For a date range 2010-2020 this requires around 15 GB of space. This is a compute-heavy step and takes approximately five hours to complete. It uses a DASK Bag to read the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:37166' processes=8 threads=16, memory=63.95 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Profiles\\Users\\bilal.hayat\\AppData\\Roaming\\Python\\Python37\\site-packages\\distributed\\worker.py:3886: UserWarning: Large object of size 2.18 MiB detected in task graph: \n",
      "  [[\"('to-parquet-c3f2c5312aa9aa1516b4f9a0fc27867a', ... 10parquet2020']\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - ERROR - Failed to kill worker process: [WinError 5] Access is denied\n",
      "distributed.nanny - ERROR - Failed to kill worker process: [WinError 5] Access is denied\n",
      "distributed.nanny - ERROR - Failed to kill worker process: [WinError 5] Access is denied\n",
      "distributed.nanny - ERROR - Failed to kill worker process: [WinError 5] Access is denied\n",
      "distributed.nanny - ERROR - Failed to kill worker process: [WinError 5] Access is denied\n",
      "distributed.nanny - ERROR - Failed to kill worker process: [WinError 5] Access is denied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4h 32min 39s\n"
     ]
    }
   ],
   "source": [
    "%time Extract.convertCrossrefJSONDumpToParquetDDF(path_to_crossref, path_to_crossref_parquet, physicalCores, virtualPerCore, start_year, end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert parquet to pickle format\n",
    "This step relates to extracting relevant columns of metadata to pickle format. To process the metadata within 64 GB RAM, it is divided into two different files. The first is used for creating the scientific networks while the second is used to display details of publications indexed by the system. It is a memory-intensive step and takes approximately an hour to complete. It creates two 8 GB binary files that can be used by subsequent phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55min\n"
     ]
    }
   ],
   "source": [
    "%time Extract.convertParquetToPickleDF (path_to_crossref_parquet, CrossrefColsDF, metadataFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51min 19s\n"
     ]
    }
   ],
   "source": [
    "%time Extract.convertParquetToPickleDF (path_to_crossref_parquet, descColsDF, metadataDesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract COCI for the provided date range in parquet format\n",
    "This step relates to creating a parquet binary file format for COCI. It is needed for the fast processing of subsequent steps. For a date range from the year 2010 to the year 2020, this requires around 45 GB of space. This is a compute-heavy step and takes approximately three hours to complete. It uses a DASK Dataframe to read the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:37458' processes=8 threads=16>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 4 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 15min 22s\n"
     ]
    }
   ],
   "source": [
    "%time Extract.convertCOCIDumpToParquetDDF (path_to_COCI, COCIColsDDF, physicalCores, virtualPerCore, path_to_COCI_parquet, start_year, end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase concerns transforming the data acquired to a format that is suitable for loading to the network processing library. The steps are dependent on their preceding steps. This phases takes around 2 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input use case name to append \n",
    "usecase=\"IND\"\n",
    "\n",
    "#Input random network Flag\n",
    "randomFlag = False\n",
    "\n",
    "#Input citation cascade details / total ego levels \n",
    "totalLevels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase= \"../\"+usecase+\"/\"\n",
    "DOIFile = usecase+\"DOI.txt\"\n",
    "DOIPkl = usecase+\"DOIs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make df\n",
    "citNetFull = usecase+str(start_year)+\"COCIFull\"+str(end_year)+\".pkl\"\n",
    "metaData = usecase+str(start_year)+\"metadata\"+str(end_year)+\".pkl\"\n",
    "citNet = usecase+str(start_year)+\"COCI\"+str(end_year)+\".pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert DOI List to pickle\n",
    "This step relates to converting the seed DOIs to pickle format. This step is needed before transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "%time Transform.getDOIsFromTxt (DOIFile, DOIPkl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter COCI using DOIs ego network\n",
    "This step relates to iteratively adding the references and citations of DOIs and updating the DOI List. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:37639' processes=8 threads=16, memory=63.95 GiB>\n",
      "Ego Nodes Edges\n",
      "1 4649 4921\n",
      "2 100756 210430\n",
      "3 7222794 16286012\n",
      "Wall time: 1h 36min 46s\n"
     ]
    }
   ],
   "source": [
    "%time Transform.filterCOCIDump (path_to_COCI_parquet, physicalCores, virtualPerCore,citNetFull, DOIPkl, totalLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter metadata using COCI DOIs\n",
    "This step relates to taking metadata of relevant DOIs. It is needed to create the corresponding venue citation network and author citation network for the publication citation network created by the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25min 8s\n"
     ]
    }
   ],
   "source": [
    "%time Transform.filterMetadata (metadataFull, citNetFull, metaData, citNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%time Transform.filterZeroOutDegNodesFromCOCI (metaData, citNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase concerns the loading of data into a network processing library using the following Equations. Scripts have been optimised to work with millions of edges with 64 GB memory. The unique list of nodes is first hashed to an increasing number representing the node labels. Edges are created between the integer labels. This phases takes a few minutes to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization for Loading\n",
    "autCitNet = usecase+\"autCOCI.pkl\"\n",
    "autCitNetLst = usecase+\"autCOCILst.pkl\"\n",
    "\n",
    "refCutoffPub = 3\n",
    "refCutoffVen = 5\n",
    "refCutoffAut = 10\n",
    "\n",
    "pubF = usecase+\"Publication\"\n",
    "venF = usecase+\"Venue\"\n",
    "autF = usecase+\"Author\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate publication citation network\n",
    "This step relates to generating the publication citation network using the COCI data. The COCI data has a few self-loops and bi-directional loops which are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes: 407632, Number of Edges: 657046\n",
      "Graph created for ../IND/Publication \n",
      "Number of Nodes: 5519, Number of Edges: 13776\n",
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "%time Load.generatePublicationCitationNetwork(pubF, metaData, citNet, randomFlag, refCutoffPub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate venue citation network\n",
    "This step relates to generating the venue citation network using the corresponding journal of the filtered publication citation network. Even though self-citations at the level of an individual or publisher are sometimes critiqued, it exists as a valid form of recognition and are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes: 1547, Number of Edges: 13782\n",
      "Graph created for ../IND/Venue \n",
      "Number of Nodes: 404, Number of Edges: 10243\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%time Load.generateVenueCitationNetwork(venF, citNet, refCutoffVen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Author disambiguation\n",
    "One of the key challenges in the bibliometric analysis is the author name disambiguation. Crossref does not provide any disambiguation like Microsoft Academic Graph \\cite{wang_microsoft_2020}. If ORCID is available in the Crossref it is recorded along with the given name and last name of the authors. If the name has multiple ORCID associated then the ORCID is used as an identifier instead of a name. This way authors within the field are disambiguated using a simple approach, similar to \\cite{milojevic_accuracy_2013}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%time Load.generateAuthorID (metaData, citNet, autCitNet, autCitNetLst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Generate author citation network\n",
    "This step relates to generating the author citation network using the corresponding authors of the filtered publication citation network. Cartesian product between the authors of citing and cited publication is made. In case of publication having a large number of authors, the list is kept up to 10 authors including the last two authors. This step is necessary to keep the network in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes: 9084, Number of Edges: 141947\n",
      "Graph created for ../IND/Author \n",
      "Number of Nodes: 2424, Number of Edges: 62852\n",
      "Wall time: 38.1 s\n"
     ]
    }
   ],
   "source": [
    "%time Load.generateAuthorCitationNetwork(autF, autCitNet, autCitNetLst, refCutoffAut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase concerns with creating an index using the following Equations. Steps are to be executed sequentially. Percentile ranks are used instead of the raw score. This phases takes a few minutes to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following files are needed in the CWD\n",
    "\n",
    "VenueGraph = usecase+\"Venue.graph\"\n",
    "VenueHash = usecase+\"Venue.hash\"\n",
    "\n",
    "PublicationGraph = usecase+\"Publication.graph\"\n",
    "PublicationHash = usecase+\"Publication.hash\"\n",
    "\n",
    "AuthorGraph = usecase+\"Author.graph\"\n",
    "AuthorHash = usecase+\"Author.hash\"\n",
    "\n",
    "# following files will be created in the CWD\n",
    "AuthorInfoCSV = usecase+\"Author.info\"\n",
    "VenueInfoCSV = usecase+\"Venue.info\"\n",
    "PublicationInfoCSV = usecase+\"Publication.info\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Calculate author and venue score using PageRank\n",
    "This step relates to indexing the venue using the PageRank score. A node with citations from a high score node also gets a high score. This recursive mechanism guarantees that not only highly cited nodes are indexed but also nodes cited by other important nodes are also indexed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Venue Ranking Completed\n",
      "Wall time: 293 ms\n"
     ]
    }
   ],
   "source": [
    "%time Index.generateVenueRank(VenueHash, VenueGraph, VenueInfoCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author Ranking Completed\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "%time Index.generateAuthorRank(AuthorHash, AuthorGraph, AuthorInfoCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Calculate publication score using author and venue score\n",
    "This step relates to indexing publications based on the two scores. First, the score of the venue where it is published and the cumulative score of which authors published it. Second, the cumulative score of venues and authors by which the publication is cited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Ranking Completed\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%time Index.generatePublicationRank(VenueInfoCSV, metaData, PublicationGraph, PublicationHash, AuthorInfoCSV, PublicationInfoCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
